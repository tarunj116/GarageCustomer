"use strict";

Object.defineProperty(exports, "__esModule", {
  value: true
});
exports.default = void 0;

var _lifeEvents = _interopRequireWildcard(require("@rpldy/life-events"));

var _shared = require("@rpldy/shared");

var _processor = _interopRequireDefault(require("./processor"));

var _consts = require("./consts");

var _utils = require("./utils");

function _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }

function _getRequireWildcardCache() { if (typeof WeakMap !== "function") return null; var cache = new WeakMap(); _getRequireWildcardCache = function () { return cache; }; return cache; }

function _interopRequireWildcard(obj) { if (obj && obj.__esModule) { return obj; } if (obj === null || typeof obj !== "object" && typeof obj !== "function") { return { default: obj }; } var cache = _getRequireWildcardCache(); if (cache && cache.has(obj)) { return cache.get(obj); } var newObj = {}; var hasPropertyDescriptor = Object.defineProperty && Object.getOwnPropertyDescriptor; for (var key in obj) { if (Object.prototype.hasOwnProperty.call(obj, key)) { var desc = hasPropertyDescriptor ? Object.getOwnPropertyDescriptor(obj, key) : null; if (desc && (desc.get || desc.set)) { Object.defineProperty(newObj, key, desc); } else { newObj[key] = obj[key]; } } } newObj.default = obj; if (cache) { cache.set(obj, newObj); } return newObj; }

const EVENT_NAMES = Object.values(_consts.UPLOADER_EVENTS);
const EXT_OUTSIDE_ENHANCER_TIME = "Uploady - uploader extensions can only be registered by enhancers",
      EXT_ALREADY_EXISTS = "Uploady - uploader extension by this name [%s] already exists";
let counter = 0;

var _default = options => {
  counter += 1;
  const uploaderId = `uploader-${counter}`;
  let enhancerTime = false;
  const extensions = {};

  _shared.logger.debugLog(`uploady.uploader: creating new instance (${uploaderId})`, {
    options,
    counter
  });

  let uploaderOptions = (0, _utils.getMandatoryOptions)(options);

  const update = updateOptions => {
    //TODO: updating concurrent and maxConcurrent means we need to update the processor - not supported yet!
    uploaderOptions = (0, _shared.merge)({}, uploaderOptions, updateOptions); //need deep merge for destination

    return uploader;
  };

  const add = (files, addOptions) => {
    const processOptions = (0, _shared.merge)({}, uploaderOptions, addOptions);

    if (processOptions.clearPendingOnAdd) {
      clearPending();
    }

    const batch = processor.addNewBatch(files, uploader.id, processOptions);
    let resultP;

    if (batch.items.length) {
      resultP = processor.runCancellable(_consts.UPLOADER_EVENTS.BATCH_ADD, batch, processOptions).then(isCancelled => {
        if (!isCancelled) {
          _shared.logger.debugLog(`uploady.uploader [${uploader.id}]: new items added - auto upload =
                        ${String(processOptions.autoUpload)}`, batch.items);

          if (processOptions.autoUpload) {
            processor.process(batch);
          }
        } else {
          batch.state = _shared.BATCH_STATES.CANCELLED;
          triggerWithUnwrap(_consts.UPLOADER_EVENTS.BATCH_CANCEL, batch);
        }
      });
    } else {
      _shared.logger.debugLog(`uploady.uploader: no items to add. batch ${batch.id} is empty. check fileFilter if this isn't intended`);
    }

    return resultP || Promise.resolve();
  };

  const clearPending = () => {
    processor.clearPendingBatches();
  };
  /**
   * process batches that weren't auto-uploaded
   */


  const getOptions = () => {
    return (0, _shared.clone)(uploaderOptions);
  };

  const registerExtension = (name, methods) => {
    (0, _shared.invariant)(enhancerTime, EXT_OUTSIDE_ENHANCER_TIME);
    (0, _shared.invariant)(!extensions[name], EXT_ALREADY_EXISTS, name);

    _shared.logger.debugLog(`uploady.uploader: registering extension: ${name.toString()}`, methods);

    extensions[name] = methods;
  };

  let {
    trigger,
    target: uploader
  } = (0, _lifeEvents.default)({
    id: uploaderId,
    update,
    add,
    upload: uploadOptions => {
      processor.processPendingBatches(uploadOptions);
    },
    abort: id => {
      processor.abort(id);
    },
    abortBatch: id => {
      processor.abortBatch(id);
    },
    getOptions,
    clearPending,
    registerExtension,
    getExtension: name => {
      return extensions[name];
    }
  }, EVENT_NAMES, {
    canAddEvents: false,
    canRemoveEvents: false
  });
  /**
   * ensures that data being exposed to client-land isnt a proxy, only pojos
   */

  const triggerWithUnwrap = (name, ...data) => {
    //delays unwrap to the very last time on trigger. Will only unwrap if there are listeners
    const lp = (0, _lifeEvents.createLifePack)(() => data.map(_utils.deepProxyUnwrap));
    return trigger(name, lp);
  };

  const cancellable = (0, _shared.triggerCancellable)(triggerWithUnwrap);

  if (uploaderOptions.enhancer) {
    enhancerTime = true;
    const enhanced = uploaderOptions.enhancer(uploader, triggerWithUnwrap);
    enhancerTime = false; //graceful handling for enhancer forgetting to return uploader

    uploader = enhanced || uploader;
  }

  const processor = (0, _processor.default)(triggerWithUnwrap, cancellable, uploaderOptions, uploader.id);
  return (0, _shared.devFreeze)(uploader);
};

exports.default = _default;